---
title: "Functional and Reproducible Code is Rare in Ecology Papers"
author: |
  | Kenneth F. Kellner
  | Michigan State University
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    mathjax: null
    toc: true
    toc_float: true
---

# Required Libraries

```{r}
suppressMessages(library(rstanarm))
suppressMessages(library(sankey))
suppressMessages(library(rmarkdown))
```

# Read in data

```{r}
# Included papers
incl <- read.csv("included_papers_final.csv")

# Journal information
jour <- read.csv("journal_data.csv")

# Merge journal data into included papers dataset
incl <- merge(incl, jour, by = c("Journal", "Year"))

# Potentially reproducible papers
repr <- read.csv("reproducible_papers_final.csv")

# Validation information
val <- read.csv("validation_final.csv")
```

# Fit models

## Model for availability of code and data

Determine the proportion of papers for which it is possible to attempt to reproduce the results. 
That is, are both data and code for a given paper available?

**Predictions:**

1. Papers in journals with data and code availability policies will be more likely to have code and data available. Covariate: `Code_Required` (implies data required also)
2. Awareness of the importance of reproducibility has been increasing. Thus, year (2018-2022) will have a positive effect on availability of both data and code. Covariate: `Year` (standardized)

We originally included another prediction:

* Papers in higher-impact journals (as measured by impact factor) will be more likely to have code and data. Covariate: `Impact_Factor`

We removed this based on reviewer feedback.

```{r}
# Create Response variable
# Are both data and code for a given paper available?
incl$Data_and_Code <- as.numeric(incl$All_Data & incl$All_Code)

# Set 2018 as year 0 instead of scaling it
incl$Year <- incl$Year - min(incl$Year, na.rm=TRUE)

# Fit GLMM
set.seed(123)
mod1 <- rstanarm::stan_glmer(
          Data_and_Code ~ Code_Required + Year + (1|Journal),
          family = binomial, data = incl, 
          chains = 4, iter = 2000, refresh = 0)
```

### Traceplot

```{r}
# Convergence
rstan::traceplot(mod1$stanfit)
```

### Posterior predictive check

```{r}
rstanarm::pp_check(mod1)
```

### Summary

```{r}
summary(mod1, probs = c(0.025, 0.975))
```

## Model for code functionality

For papers with code and data available, are we able to successfully run the code?

**Predictions:**

1. We expect that the longer and more complex an analysis (as measured crudely in lines of code), the less likely we will be to run the code successfully. The more code you have, the more chances there are to introduce errors. Covariate: `Code_Lines`
2. The more outside packages the code depends on, the less likely the code will be to run. This includes all dependencies of packages called in the script. Covariate: `Libraries`
3. Code provided in a format designed for reproducibility, such as R markdown, will be more likely to run successfully than code in a text file (e.g., .R). Code in text files will be more likely to run than code shared in a PDF or Word document. Covariate: `Code_Format`

We added a 4th prediction based on reviewer feedback:

4. There will be a positive effect of year on the probability a paper's code runs, potentially due to increasing awareness of good coding practices or higher journal standards.

```{r}
# Response variable
table(repr$All_Code_Runs)

# Create code format covariate
code_format <- rep(NA, nrow(repr))
code_format[repr$Word == 1 | repr$PDF == 1] <- "PDF/Word"
code_format[repr$Script == 1] <- "R script"
code_format[repr$RMarkdown == 1 | repr$Other == 1] <- "Rmd/other" # "Other" here is R package
repr$Code_Format <- factor(code_format, levels=c("R script", "Rmd/other", "PDF/Word"))
# Set year baseline to 2018
repr$Year <- repr$Year - min(repr$Year)
table(repr$Code_Format)

# Fit GLMM
set.seed(123)
mod2 <- rstanarm::stan_glmer(
          All_Code_Runs ~ scale(Code_Lines) + scale(Libraries) + Code_Format + Year + (1|Journal),
          family = binomial, data = repr,
          chains = 4, iter = 2000, refresh = 0)
```

### Traceplot

```{r}
rstan::traceplot(mod2$stanfit)
```

### Posterior predictive check

```{r}
rstanarm::pp_check(mod2)
```

### Summary

```{r}
summary(mod2, probs = c(0.025, 0.975))
```

\pagebreak

# Results text

```{r, echo=FALSE}
tabj <- table(incl$Journal)
rng_j <- range(tabj)

tab_y <- table(incl$Year)
rng_y <- range(tab_y)

n_incl <- nrow(incl)
```

We identified `r nrow(incl)+3` papers from `r length(tabj)` journals that met the inclusion criteria.
*Journal of Ecology* was excluded from the final dataset because we found only 2 relevant papers.
Three papers were excluded because they were written by a co-author of this study, resulting a final sample size of `r n_incl`.
The mean number of papers per journal was `r round(mean(tabj))` and the range was `r paste0(rng_j[1], "—", rng_j[2])`.
There were a similar number of papers from each year 2018—2022 (mean `r round(mean(tab_y))`, range `r paste0(rng_y[1], "—", rng_y[2])`).

## Data and code availability

```{r, echo = FALSE}
n_data <- sum(incl$All_Data & !incl$All_Code)
n_code <- sum(!incl$All_Data & incl$All_Code)
neither <- sum(!incl$All_Data & !incl$All_Code)

pct_data <- round((n_data/n_incl)*100)
pct_code <- round((n_code/n_incl)*100)
pct_neither <- round((neither/n_incl)*100)

possible_reproduce <- incl$All_Data & incl$All_Code
impl_repr <- sum(possible_reproduce)
n_repr <- nrow(repr)
stopifnot(impl_repr == n_repr)
pct_repr <- round(n_repr / n_incl * 100)

pct_appen <- round(mean(repr$Appendix)*100)
pct_dryad <- round(mean(repr$Dryad)*100)
pct_zenodo <- round(mean(repr$Zenodo)*100)

not_incl <- incl[!possible_reproduce,]
mc <- sum(not_incl$Model_Code_Only)
ml <- sum(not_incl$Missing_Broken_Link)
ma <- sum(not_incl$Missing_Appendix)

tab_y <- table(incl$Year, possible_reproduce)
# year 4 = 2022
pct_22 <- round(tab_y['4','TRUE'] / rowSums(tab_y)['4'] * 100)

year_post <- as.matrix(mod1)[,"Year"]
pct_pos <- round(mean(year_post > 0)*100)

year_ef <- round((exp(coef(mod1)$Journal$Year[1]) - 1) * 100)
year_ci <- round((exp(quantile(year_post, c(0.025, 0.975))) - 1) * 100)
```

Of these `r n_incl` papers, `r paste0(n_repr, " (", pct_repr, "%)")` made both data and code available and thus were potentially reproducible (Figure 1).
Of the remainder, `r paste0(n_data, " (",pct_data,"%)")` had only data, `r paste0(n_code, " (",pct_code,"%)")` had only code, and `r paste0(neither, " (",pct_neither,"%)")` had neither data nor code.
The most common repositories used were paper appendices (`r paste0(pct_appen, "%")`), Zenodo (`r paste0(pct_zenodo, "%")`), and Dryad (`r paste0(pct_dryad, "%")`).
Common data and code sharing problems (excluding not sharing code or data at all) included supplying only BUGS/Stan model code (`r mc` papers), missing or broken links to outside repositories (`r ml` papers), and missing or incomplete appendices referenced in the text (`r ma` papers).

There was a positive effect of year on the probability both code and data were available, with `r pct_pos`% of the posterior samples for the parameter greater than 0. 
A one-year increase in published date corresponded to a `r paste0(year_ef, "% (95% credible interval ", year_ci[1], "% - ", year_ci[2],"%)")` increase in the odds a paper had both code and data available (Table 1, Figure 2).
The probability a paper had both data and code accessible did not differ based on code sharing policy (Table 1, Figure 2).

## Code functionality

```{r, echo=FALSE}
n_excl <- sum(is.na(repr$All_Code_Runs))
pct_excl <- round(n_excl/n_repr*100)
n_func <- sum(repr$All_Code_Runs, na.rm=TRUE)
n_kept <- n_repr - n_excl
pct_func <- round(n_func / n_kept * 100)

kept_sub <- repr[!is.na(repr$All_Code_Runs),]

n_depr <- sum(kept_sub$Depr_Packages)
n_file <- sum(kept_sub$Miss_File)
n_obj <- sum(kept_sub$Miss_Object)
n_lib <- sum(kept_sub$Miss_Library)

pct_com <- round(mean(kept_sub$Commented, na.rm = TRUE) * 100)

cl_mn <- round(mean(kept_sub$Code_Lines))
cl_sd <- round(sd(kept_sub$Code_Lines))

pkg_mn <- round(mean(kept_sub$Libraries))
pkg_sd <- round(sd(kept_sub$Libraries))

n_script <- sum(kept_sub$Script)
pct_script <- round(n_script / n_kept * 100)

n_mark <- sum(kept_sub$RMarkdown)
pct_mark <- round(n_mark / n_kept * 100)

# Effect checks
s2 <- as.data.frame(summary(mod2, probs=c(0.025,0.975)))
code_lines <- s2["scale(Code_Lines)",]
code_lines_neg <- code_lines$`2.5%` < 0 & code_lines$`97.5%` < 0
stopifnot(code_lines_neg)
code_lines_ef <- round(code_lines$mean, 2)
code_lines_ci <- paste0(round(code_lines$`2.5%`, 2), " — ", round(code_lines$`97.5%`, 2))
code_sd <- round(sd(repr$Code_Lines, na.rm=TRUE))
nd <- data.frame(Code_Lines = c(mean(repr$Code_Lines, na.rm=TRUE), 
                                mean(repr$Code_Lines, na.rm=TRUE) + sd(repr$Code_Lines, na.rm=TRUE)),
                 Libraries = median(repr$Libraries, na.rm = TRUE),
                 Code_Format = factor("R script", levels=levels(repr$Code_Format)),
                 Year = median(repr$Year))
code_post <- posterior_epred(mod2, newdata=nd, re.form=NA)
code_pr <- apply(code_post, 2, median)
```

Of the `r n_repr` potentially reproducible papers, we excluded `r n_excl` (`r  pct_excl`%) from further analysis due to issues we blamed on ourselves (extremely long runtimes, RAM limitations, etc.)
On average, papers shared `r cl_mn` ± `r cl_sd` (mean ± standard deviation)  lines of code and depended on `r pkg_mn` ± `r pkg_sd` R packages.
The majority (`r n_script`, `r pct_script`%) provided code primarily in R script format.
Only a few papers used primarily R Markdown or similar tools (`r n_mark`, `r pct_mark`%).

We were able to successfully execute all the provided code for `r n_func` of `r n_kept` papers (`r pct_func`%; Figure 1).
The most common reasons that code failed to run (there could be multiple reasons) were missing or misnamed files (`r n_file` papers), reliance on deprecated R packages (`r n_depr` papers), and missing R objects (`r n_obj` papers; Figure 1).

The number of lines of code had a negative effect on the probability a paper's code ran successfully, matching our prediction (Table 2, Figure 3).
With other covariates held at median or reference levels, a paper with the mean number of lines of code (`r round(nd$Code_Lines[1])`) had a `r round(code_pr[1],2)` probability of the code running, while a paper with lines of code one standard deviation above the mean (`r round(nd$Code_Lines[2])`) had a `r round(code_pr[2],2)` probability of the code running.
The probability that a paper's code ran successfully was not related to the number of packages used, code format, or year (Table 2,  Figure 3).

## Matching code output

Note: we did not include these results in the final version of the paper following reviewer feedback.

```{r, echo = FALSE}
n_long <- sum(repr$Runtime_Too_Long & repr$All_Code_Runs, na.rm=TRUE)
pct_long <- round(n_long / n_func * 100)
n_check <- sum(!is.na(repr$Outputs_Match))
n_match <- sum(repr$Outputs_Match, na.rm = TRUE)
pct_match <- round(n_match/n_check*100)
```

Of the `r n_func` papers with functional code, `r n_long` (`r pct_long`%) would have taken too long to fully run due to extensive simulations, MCMC iterations, etc. (Figure 1).
That left `r n_check` papers that we could fully check for reproducibility.
Of these, `r n_match` (`r pct_match`%) reproduced the results reported in the associated paper (Figure 1).
Code from an additional 2 papers produced results similar to the text that probably would have matched if the code had set a random seed.
Given this small sample size, we did not attempt to test our predictions for matching code output with this dataset.

## Accuracy check

```{r, echo = FALSE}
nval <- nrow(val)
non_match <- sum(grepl("False", val$Validation_Notes))
stopifnot(non_match == 2)
match_pct <- round((nval-non_match)/nval * 100)
```

Of the `r nval` papers for which two authors executed code, we found the same results for `r nval - non_match` papers (`r match_pct`%).
One paper was a false positive (error reported when code actually ran) and one was a false negative (code reported running but actually had an error). 
These papers were corrected in the final dataset used for analysis.
One additional disagreement was likely due to differences in computing environment and was ignored.

# Tables

### Table 1

Estimates from from the generalized linear mixed model of probability a paper provided both code and data, as a function of journal code policy and year.
Intercepts were random by journal.

```{r, echo = FALSE}
tab1 <- as.data.frame(summary(mod1, probs = c(0.025, 0.975)))
tab1$`95% CI` <- paste0("(",round(tab1$`2.5%`, 2), " — ", round(tab1$`97.5%`, 2), ")")
tab1 <- tab1[!grepl("b[", rownames(tab1), fixed=TRUE),]
tab1 <- tab1[1:4,]
tab1$Parameter <- c("Intercept","Code required", "Year", "Journal SD")
tab1$Estimate <- round(tab1$mean, 2)
tab1 <- tab1[,c("Parameter","Estimate","95% CI")]
rownames(tab1) <- NULL

knitr::kable(tab1)

write.csv(tab1, "Table_1.csv", row.names=FALSE)
```

### Table 2

Estimates from from the generalized linear mixed model of probability a paper's code ran successfully, as a function of number of lines of code, number of R packages required, and code format (relative to a reference level of R script).
Intercepts were random by journal.

```{r, echo = FALSE}
tab2 <- as.data.frame(summary(mod2, probs = c(0.025, 0.975)))
tab2$`95% CI` <- paste0("(",round(tab2$`2.5%`, 2), " — ", round(tab2$`97.5%`, 2), ")")
tab2 <- tab2[!grepl("b[", rownames(tab2), fixed=TRUE),]
tab2 <- tab2[1:7,]
tab2$Parameter <- c("Intercept","Code lines", "Libraries", "Rmd/other", "PDF/Word", "Year", "Journal SD")
tab2$Estimate <- round(tab2$mean, 2)
tab2 <- tab2[,c("Parameter","Estimate","95% CI")]
rownames(tab2) <- NULL

knitr::kable(tab2)
write.csv(tab2, "Table_2.csv", row.names=FALSE)
```

\pagebreak


# Figures

### Figure 1

Sankey diagram categorizing code availability and code execution status of papers included in the analysis.
Node labels (and the number of papers at each node) are below the associated node.
Green color indicates a positive outcome (code/data available, code runs) and yellow indicates a negative outcome (no code available, code does not run).

```{r, echo = FALSE, message = FALSE, fig.height=7}

# Set up first node and split
all_papers <- nrow(incl)
all_papers_lab <- paste0("All papers\n(",all_papers,")")

code_and_data <- sum(incl$All_Data & incl$All_Code)
code_and_data_lab <- paste0("\nHas code/\ndata (", code_and_data, ")")
incomplete <- all_papers - code_and_data
inc_lab <- paste0("\nMissing\ncode/data\n(",incomplete,")")

split1 <- data.frame(start_node = rep(all_papers_lab, 2),
                     end_node = c(code_and_data_lab, inc_lab),
                     weight = c(code_and_data, incomplete), 
                     colorstyle="col", col=c("forestgreen", "goldenrod"))

# Set up second node and split
code_runs <- sum(repr$All_Code_Runs, na.rm=TRUE)
code_runs_lab <- paste0("\nCode\nruns (",code_runs,")")
other_issues <- sum(is.na(repr$All_Code_Runs))
other_issues_lab <- paste0("\nOther\nissues (", other_issues, ")")
doesnt_run <- code_and_data - other_issues - code_runs
doesnt_run_lab <- paste0("\nCode\nerrors (",doesnt_run,")") 


split2 <- data.frame(start_node = rep(code_and_data_lab, 3),
                     end_node = c(code_runs_lab, doesnt_run_lab, other_issues_lab),
                     weight = c(code_runs, doesnt_run, other_issues),
                     colorstyle = "col", col=c("forestgreen","goldenrod","gray"))

# Set up third node and splits
data_sub_norun <- repr[repr$All_Code_Runs==0 & !is.na(repr$All_Code_Runs),]

dep_pkg <- sum(data_sub_norun$Depr_Packages == 1)
dep_pkg_lab <- paste0("Deprecated\npackage\n(", dep_pkg, ")")
miss_file <- sum(data_sub_norun$Miss_File == 1 & data_sub_norun$Depr_Packages == 0)
miss_file_lab <- paste0("File issue\n(", miss_file, ")")
miss_obj <- sum(data_sub_norun$Miss_Object == 1 & data_sub_norun$Depr_Packages == 0)
miss_obj_lab <- paste0("Missing R\nobject (", miss_obj, ")")
other_error <- doesnt_run - dep_pkg - miss_file - miss_obj
other_error_lab <- paste0("Other error\n(", other_error, ")")

split3 <- data.frame(start_node = rep(doesnt_run_lab, 4),
                     end_node = c(dep_pkg_lab, miss_file_lab,
                                  miss_obj_lab, other_error_lab),
                     weight= c(dep_pkg, miss_file, miss_obj, other_error),
                     colorstyle = "col", col="goldenrod")

# Combine everything
all_splits <- rbind(split1, split2, split3)

# Specify colors of nodes
nodes <- data.frame(id = c(all_papers_lab, code_and_data_lab, 
                           inc_lab, code_runs_lab, doesnt_run_lab,
                           other_issues_lab, 
                           dep_pkg_lab, miss_file_lab,
                           miss_obj_lab, other_error_lab),
                    col = c("forestgreen", "forestgreen", "goldenrod", 
                            "forestgreen", "goldenrod", "gray",
                            "goldenrod","goldenrod","goldenrod","goldenrod"),
                    cex=1.1, adjy=0.9)

# Make plot input
plot_input <- make_sankey(nodes = nodes, edges = all_splits)

sankey(plot_input, mar = c(0, 3.2, 0, 4.2))

tiff("Figure_1.tiff", height=7, width=7, units='in', res=300, compression='lzw')
sankey(plot_input, mar = c(0, 3.2, 0, 4.2))
nul <- dev.off()
```

###  Figure 2

Effects of journal code policy (a), year (b), and journal (c) on proportion of papers that provided complete data and code.
Black points and lines represent estimates and 95% confidence intervals based on the raw data, and gold represents predictions from the model of data and code availability.

```{r, echo = FALSE, fig.height=7}

fig2 <- function(){

par(mar=c(4,2,1,1), oma = c(1,3,0,0))
layout(matrix(c(1, 2,
                3, 3), ncol=2, nrow=2, byrow=TRUE))

# RHS formula
form <- ~Code_Required + scale(Year)

# Code required plot
# Calculate proportion for each code requirement option
tab_req <- table(incl$Code_Required, incl$Data_and_Code)
n <- rowSums(tab_req)
p <- tab_req[,2] / n

# Plot raw data
plot(1:2-0.1, p, ylim = c(0, 0.87), pch=19,
     ylab="Prop. with code and data", cex=1.3, cex.axis=1.2,
     xlab = "Journal code policy", xlim=c(0.5, 2.5), xaxt='n', cex.lab=1.2)
axis(1, at = 1:2, labels = c("Not required", "Required"), cex.axis=1.2)

legend('topright', legend = c("Raw data", "Model prediction"),
       lty = 1, col = c("black", "goldenrod"))

text(0.7, 0.8, "(a)", cex=2)

mtext("Proportion papers with code and data, and 95% CI", side = 2, line = 1,
      outer = TRUE)

# Plot model predictions
nd <- data.frame(Code_Required = c(0, 1), Year = median(incl$Year))

pr_post <- rstanarm::posterior_epred(mod1, newdata = nd, re.form=NA)
est <- apply(pr_post, 2, mean)
low_fit <- apply(pr_post, 2, quantile, 0.025)
up_fit <- apply(pr_post, 2, quantile, 0.975)

points(1:2+0.1, est, cex = 1.3, pch = 19, col='goldenrod')

# Error bars for model prediction
segments(1:2+0.1, low_fit, 1:2+0.1, up_fit, col='goldenrod')
bw <- 0.05
for (i in 1:2){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

# Error  bars for raw data
# 95% CI
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
segments(1:2-0.1, low, 1:2-0.1, up)
bw <- 0.05
for (i in 1:2){
  segments(i-0.1-bw, low[i], i-0.1+bw, low[i])
  segments(i-0.1-bw, up[i], i-0.1+bw, up[i])
}


# Year effect plot
yrs <- 2018:2022

# Calculate proportion in each year
tab_year <- table(incl$Year, incl$Data_and_Code)
n <- rowSums(tab_year)
p <- tab_year[,2] / n

# Plot raw data
plot(yrs, p, ylim = c(0, 0.87), pch=19,
     ylab="Prop. with code and data", cex=1.3, cex.axis=1.2,
     xlab = "Year", xlim=c(2017.75, 2022.25), cex.lab=1.2)

text(2018.25, 0.8, "(b)", cex=2)

# Plot model predictions
nd <- data.frame(Code_Required = 0, Year = 2018:2022 - 2018, 
                 Impact_Factor = median(incl$Impact_Factor))

pr_post <- rstanarm::posterior_epred(mod1, newdata = nd, re.form=NA)
est <- apply(pr_post, 2, mean)
low_fit <- apply(pr_post, 2, quantile, 0.025)
up_fit <- apply(pr_post, 2, quantile, 0.975)

polygon(c(yrs, rev(yrs)), c(low_fit, rev(up_fit)), col='lightgoldenrod1', border=NA)
lines(yrs, est, lty = 2, col='goldenrod')

# Re-plot raw data on top
points(yrs, p, pch = 19, cex = 1.3)

# Error  bars for raw data
# 95% CI
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
segments(yrs, low, yrs, up)
bw <- 0.1
for (i in 1:5){
  segments(yrs[i]-bw, low[i], yrs[i]+bw, low[i])
  segments(yrs[i]-bw, up[i], yrs[i]+bw, up[i])
}

# Journal plot

# Calculate proportion for each journal
tab_journ <- table(incl$Journal, incl$Data_and_Code)
nj <- nrow(tab_journ)
n <- rowSums(tab_journ)
p <- tab_journ[,2] / n

abbrev <- c("JWM","E&E","ECOS","PLOS","SREP","ECOL","BC","CB","MEE")

# Plot raw data
plot(1:nj-0.1, p, ylim = c(0, 0.87), pch=19,
     ylab="Prop. with code and data", cex=1.3, cex.axis=1.2,
     xlab = "Journal", cex.lab = 1.2,
     xlim = c(0.5, nj+0.5), xaxt='n')
axis(1, at=1:nj, labels=abbrev)

text(0.7, 0.8, "(c)", cex=2)

# Plot model predictions
nd <- data.frame(Code_Required = 0, Year = median(incl$Year), 
                 Journal = unique(incl$Journal))

pr_post <- rstanarm::posterior_epred(mod1, newdata = nd)
est <- apply(pr_post, 2, mean)
low_fit <- apply(pr_post, 2, quantile, 0.025)
up_fit <- apply(pr_post, 2, quantile, 0.975)

# Plot model prediction
points(1:nj+0.1, est, pch = 19, cex = 1.3, col='goldenrod')
segments(1:nj+0.1, low_fit, 1:nj+0.1, up_fit, col='goldenrod')
bw <- 0.1
for (i in 1:nj){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

# Re-plot raw data on top
#points(1:nj, p, pch = 19, cex = 1.3)

# Error  bars for raw data
# 95% CI
xval <- 1:nj-0.1
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
segments(xval, low, xval, up)
bw <- 0.1
for (i in 1:length(xval)){
  #text(xval[i], up[i] + 0.05, abbrev[i])
  segments(xval[i]-bw, low[i], xval[i]+bw, low[i])
  segments(xval[i]-bw, up[i], xval[i]+bw, up[i])
}
}

fig2()

tiff("Figure_2.tiff", height=7, width=7, units='in', res=300,
     compression='lzw')
fig2()
nul <- dev.off()
```

\pagebreak

### Figure 3

Effects of code lines (a), number of required R packages (b), code format (c), and year (d) on proportion of papers for which provided code ran successfully.
Black points and lines represent estimates and 95% confidence intervals based on the raw data, and gold represents predictions from the model of code functionality.

```{r, echo = FALSE, fig.height = 3.5, fig.width=7}

fig3 <- function(){

par(mar=c(4,2,1,1), oma = c(1,3,0,0), mfrow=c(2,2))

form <- ~scale(Code_Lines) + scale(Libraries) + Code_Format + Year

# Code Lines plot
line_cat <- cut(repr$Code_Lines, c(0, 1000, 2000, 3000, 4000, 50000),
                labels = c("0-1k", "1k-2k", "2k-3k", "3k-4k",
                           "4k+"))

lintab <- table(line_cat, repr$All_Code_Runs)

n <- rowSums(lintab)
p <- lintab[,2] / n
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
xval <- levels(line_cat)

xpos <- 1:length(xval)

plot(xpos-0.1, p, ylim = c(0, 0.8), xlim = c(0.75, 5.25), pch=19,
     ylab="Prop. running code", cex=1.3, cex.lab=1.2,
     xlab = "Code lines", xaxt='n')
axis(1, at=xpos, labels=xval)

legend('topright', legend = c("Raw data", "Model prediction"),
       lty = 1, col = c("black", "goldenrod"))

text(1.2, 0.77, "(a)", cex=2)

mtext("Prop. running code and 95% CI", side = 2, line = 1,
      outer = TRUE)

# Error bars for raw data
segments(xpos-0.1, low, xpos-0.1, up)
bw <- 0.05
n <- c(paste0("n = ", n[1]), n[2:length(n)])
for (i in 1:5){
  #text(xpos[i]-0.1, up[i] + 0.03, n[i])
  segments(xpos[i]-bw-0.1, low[i], xpos[i]+bw-0.1, low[i])
  segments(xpos[i]-bw-0.1, up[i], xpos[i]+bw-0.1, up[i])
}

# Model prediction at midpoints of bins
nd <- data.frame(Code_Lines = c(500, 1500, 2500, 3500, 4500),
                 Libraries = median(repr$Libraries, na.rm = TRUE),
                 Code_Format = factor("R script", levels=levels(repr$Code_Format)),
                 Year = median(repr$Year))

pr_post <- rstanarm::posterior_epred(mod2, newdata = nd, re.form=NA)
est <- apply(pr_post, 2, mean)
low_fit <- apply(pr_post, 2, quantile, 0.025)
up_fit <- apply(pr_post, 2, quantile, 0.975)

points(1:5+0.1, est, cex = 1.3, pch = 19, col='goldenrod')
# Error bars for model prediction
segments(1:5+0.1, low_fit, 1:5+0.1, up_fit, col='goldenrod')
bw <- 0.05
for (i in 1:5){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

# Libraries
lib_cat <- cut(repr$Libraries, c(0, 50, 100, 150, 1000),
                labels = c("0-50", "51-100", "101-150", "150+"))

libtab <- table(lib_cat, repr$All_Code_Runs)

n <- rowSums(libtab)
p <- libtab[,2] / n
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
xval <- levels(lib_cat)

xpos <- 1:length(xval)

# Raw data plot
plot(xpos-0.1, p, ylim = c(0, 0.8), xlim = c(0.75, 4.25), pch=19,
     ylab="Prop. running code", cex=1.3, cex.lab=1.2,
     xlab = "Package dependencies", xaxt='n')
axis(1, at=xpos, labels=xval)

text(1.15, 0.77, "(b)", cex=2)

segments(xpos-0.1, low, xpos-0.1, up)
bw <- 0.1
n <- c(paste0("n = ", n[1]), n[2:length(n)])
for (i in 1:5){
  #text(xpos[i], up[i] + 0.03, n[i])
  segments(xpos[i]-bw-0.1, low[i], xpos[i]+bw-0.1, low[i])
  segments(xpos[i]-bw-0.1, up[i], xpos[i]+bw-0.1, up[i])
}

# Model prediction at midpoints of bins
nd <- data.frame(Code_Lines = median(repr$Code_Lines, na.rm = TRUE),
                 Libraries = c(25, 75, 125, 175),
                 Code_Format = factor("R script", levels=levels(repr$Code_Format)),
                 Year = median(repr$Year))

pr_post <- rstanarm::posterior_epred(mod2, newdata = nd, re.form=NA)

est <- apply(pr_post, 2, mean)
low_fit <- apply(pr_post, 2, quantile, 0.025)
up_fit <- apply(pr_post, 2, quantile, 0.975)
points(1:4+0.1, est, cex = 1.3, pch = 19, col='goldenrod')

# Error bars for model prediction
segments(1:4+0.1, low_fit, 1:4+0.1, up_fit, col='goldenrod')
bw <- 0.05
for (i in 1:4){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

# Code format
code_format <- rep(NA, nrow(repr))
code_format[repr$Word == 1 | repr$PDF == 1] <- "PDF/Word"
code_format[repr$Script == 1] <- "R script"
code_format[repr$RMarkdown == 1 | repr$Other == 1] <- "Rmd/other"
code_format <- factor(code_format, levels=c("R script", "Rmd/other",
                                            "PDF/Word"))

formtab <- table(code_format, repr$All_Code_Runs)

n <- rowSums(formtab)
p <- formtab[,2] / n
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
xval <- levels(code_format)

xpos <- 1:length(xval)

plot(xpos-0.1, p, ylim = c(0, 0.8), xlim = c(0.75, 3.25), pch=19,
     ylab="Prop. running code", cex=1.3, cex.lab=1.2,
     xlab = "Code format", xaxt='n')
axis(1, at=xpos, labels=xval)

text(1, 0.77, "(c)", cex=2)

segments(xpos-0.1, low, xpos-0.1, up)
bw <- 0.05
n <- c(paste0("n = ", n[1]), n[2:length(n)])
for (i in 1:3){
  #text(xpos[i], up[i] + 0.03, n[i])
  segments(xpos[i]-bw-0.1, low[i], xpos[i]+bw-0.1, low[i])
  segments(xpos[i]-bw-0.1, up[i], xpos[i]+bw-0.1, up[i])
}

# Model predictions
nd <- data.frame(Code_Lines = median(repr$Code_Lines, na.rm = TRUE),
                 Libraries = median(repr$Libraries, na.rm = TRUE),
                 Code_Format = factor(levels(repr$Code_Format),
                                      levels = levels(repr$Code_Format)),
                 Year = median(repr$Year))

pr_post <- rstanarm::posterior_epred(mod2, newdata = nd, re.form=NA)
est <- apply(pr_post, 2, mean)
low_fit <- apply(pr_post, 2, quantile, 0.025)
up_fit <- apply(pr_post, 2, quantile, 0.975)
points(1:3+0.1, est, cex = 1.3, pch = 19, col='goldenrod')

# Error bars for model prediction
segments(1:3+0.1, low_fit, 1:3+0.1, up_fit, col='goldenrod')
bw <- 0.05
for (i in 1:3){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

# Year effect plot
yrs <- 2018:2022

# Calculate proportion in each year
tab_year <- table(repr$Year, repr$All_Code_Runs)
n <- rowSums(tab_year)
p <- tab_year[,2] / n

# Plot raw data
plot(yrs, p, ylim = c(0, 0.8), pch=19,
     ylab="", cex=1.3, cex.axis=1.2,
     xlab = "Year", xlim=c(2017.75, 2022.25), cex.lab=1.2)

text(2018.25, 0.77, "(d)", cex=2)

# Plot model predictions
nd <- data.frame(Code_Lines = median(repr$Code_Lines, na.rm = TRUE),
                 Libraries = median(repr$Libraries, na.rm = TRUE),
                 Code_Format = factor("R script", levels=levels(repr$Code_Format)),
                 Year = yrs - 2018)

pr_post <- rstanarm::posterior_epred(mod2, newdata = nd, re.form=NA)
est <- apply(pr_post, 2, mean)
low_fit <- apply(pr_post, 2, quantile, 0.025)
up_fit <- apply(pr_post, 2, quantile, 0.975)

polygon(c(yrs, rev(yrs)), c(low_fit, rev(up_fit)), col='lightgoldenrod1', border=NA)
lines(yrs, est, lty = 2, col='goldenrod')

# Re-plot raw data on top
points(yrs, p, pch = 19, cex = 1.3)

# Error  bars for raw data
# 95% CI
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
segments(yrs, low, yrs, up)
bw <- 0.1
for (i in 1:5){
  segments(yrs[i]-bw, low[i], yrs[i]+bw, low[i])
  segments(yrs[i]-bw, up[i], yrs[i]+bw, up[i])
}


}

fig3()

tiff("Figure_3.tiff", height=7, width=7, units='in', res=300,
     compression='lzw')
fig3()
nul <- dev.off()
```

# Session Information

```{r}
sessionInfo()
```
